<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
	Modified by Kourosh Meshgi
-->
<html>
	<head>
		<title>Homepage of KOUROSH MESHGI</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->

	
		<!-- Vendor CSS Files -->
		<!-- <link href="assets/vendor/aos/aos.css" rel="stylesheet"> -->
		<!-- <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet"> -->
		<link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
		<link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
		<link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
		<link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
		
		<!-- Template Main CSS File -->
		<link href="assets/css/style.css" rel="stylesheet">
		
		<!-- =======================================================
		* Template Name: MyResume - v4.8.1
		* Template URL: https://bootstrapmade.com/free-html-bootstrap-template-my-resume/
		* Author: BootstrapMade.com
		* License: https://bootstrapmade.com/license/
		======================================================== -->
		<!-- <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js" type="text/javascript"></script> -->
		<script src="js/functions.js" type="text/javascript"></script>
	</head>
	<body>

			<!-- ======= Mobile nav toggle button ======= -->
			<!--
			<i class="bi bi-list mobile-nav-toggle d-xl-none"></i>
			<header id="header2" class="d-flex flex-column justify-content-center">

				<nav id="navbar" class="navbar nav-menu">
				<ul>
					<li><a href="#about" class="nav-link scrollto"><i class="bx bx-user"></i> <span>About</span></a></li>
					<li><a href="#publication" class="nav-link scrollto"><i class="bx bx-bong"></i> <span>Publications</span></a></li>
					<li><a href="#activities" class="nav-link scrollto"><i class="bx bx-book-bookmark"></i> <span>Activities</span></a></li>
					<li><a href="#projects" class="nav-link scrollto"><i class="bx bx-book-content"></i> <span>Projects</span></a></li>
					<li><a href="#contact" class="nav-link scrollto"><i class="bx bx-envelope"></i> <span>Contact</span></a></li>
				</ul>
				</nav>
			</header> -->

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">Homepage of <strong>Kourosh Meshgi</strong></a>
									<ul class="icons">
										<li><a href="https://www.twitter.com/kouroshmeshgi" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="http://jp.linkedin.com/in/kouroshmeshgi" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://github.com/meshgi/" class="icon fa-github"><span class="label">GitHub</span></a></li>
										<li><a href="http://scholar.google.com/citations?user=WejG0Z8AAAAJ&hl=en" class="icon fa-mortar-board"><span class="label">Google Scholar</span></a></li>
										<li><a href="https://www.semanticscholar.org/author/Kourosh-Meshgi/2146623" class="icon fa-paper-plane "><span class="label">Microsoft Academic research</span></a></li>
										<li><a href="http://orcid.org/0000-0001-7734-6104" class="icon fa-user"><span class="label">Orcid</span></a></li>
										<li><a href="https://www.youtube.com/playlist?list=PLxJOR8n0DcHciCixUD5259MRC3YrEAE3G" class="icon fa-youtube-play"><span class="label">YouTube</span></a></li>
										
									</ul>
								</header>

							<!-- Banner -->
								<section id="about">
									<div class="content">
										<header>
											<h1>Kourosh Meshgi</h1>
											<p>RESEARCH SCIENTIST @ <a href="http://www.riken.jp/en/research/labs/aip/" class="logo">RIKEN CENTER OF ADVANCED INTELLIGENCE PROJECT (AIP)</a></p>
										</header>
										<p>I am currently a research scientist for the <a href="https://aip.riken.jp/labs/goalorient_tech/lang_inf_access_tech/?lang=en">Language Information Access Technology (LIAT) team </a>, <a href="http://www.riken.jp/en/research/labs/aip/" class="logo">RIKEN AIP</a> under the supervision of Prof. <a href="https://nlp.cs.nyu.edu/sekine/">Satoshi Sekine</a>. I am based in Seattle and have extensive knowledge in machine learning, particularly deep learning and generative AI, not only fine-tuning models but also designing and developing novel architectures and publishing them. 
											I work on transferring my knowledge from CV to NLP to help knowledge extraction from text, constructing a bridging between large language models (LLMs) and CV tasks, structuring it for higher-level inferences using multi-task learning, as well as delving into the realm of transformers and employing them to solve problems quickly and reliably. Before that, I was a postdoctoral fellow at the <a href="http://www.i.kyoto-u.ac.jp/en/">Graduate School of Informatics</a> at <a href="http://www.kyoto-u.ac.jp/en">Kyoto University</a>, where as a member of the <a href="http://ishiilab.jp/kyoto/en/">Ishii lab</a>, I worked with <a href="http://ishiilab.jp/member/ishii/">Prof. Shin Ishii</a> and <a href="http://ishiilab.jp/member/oba/">Dr. Shigeyuki Oba</a> on the use of active learning in visual tracking and scene understanding, funded by the project of strategic advancement of multi-purpose ultra-human robot and artificial intelligence technologies (SAMURAI) from the New Energy and Industrial Technology Development Organization (<a href="http://www.nedo.go.jp/activities/ZZJP_100106.html">NEDO</a>) and <a href=" https://brain-hpc.jp/">Post-Kei project</a>. I am also collaborating with <a href="http://www.qbic.riken.jp/english/research/outline/lab-11.html">Biochemical Simulation Lab</a> from <a href="https://aics.riken.jp/en/postk/project.html">RIKEN QBiC</a>, <a href="https://www.riken.jp/en/research/labs/aip/goalorient_tech/machine_intell_med_eng/index.html">RIKEN AIP's Machine Intelligence for Medical Engineering Team</a>, and <a href="https://groups.oist.jp/ncu">Neural Computation Unitof OIST</a> on the Exploratory Challenges of Post-K project. I also collaborate with <a href="http://www.riken.jp/en/research/labs/aip/ai_soc/hum_ai_comm/">Human-AI Communication team</a> of <a href="http://www.riken.jp/en/research/labs/aip/">RIKEN AIP</a> under the supervision of <a href="https://www.ii.ist.i.kyoto-u.ac.jp/?page_id=181">Prof. Toyoaki Nishida</a>.</p> 
										<p>I finished my Ph.D. under the supervision of Prof. Shin Ishii in the same lab, where I worked on occlusion-aware visual tracking, 3D object reconstruction, and imitation learning. In particular, my Ph.D. dissertation was about particle filter-based tracking to handle persistent and complex occlusions by proposing novel occlusion-aware appearance model and context-aware motion model in this framework.
										</p>
										<p><strong>Bio:</strong> Kourosh Meshgi received his B.Sc. and M.Sc. in Hardware Engineering (2008) and Artificial Intelligence (2010) respectively, from Tehran Polytechnic and his Ph.D. in Informatics in 2015 from Kyoto University. He is currently a research scientist at RIKEN AIP (remotely working from Seattle, WA) and a guest researcher at Kyoto University. His research interests include machine learning, natural language processing, computer vision, robotics, and computational linguistics. Dr. Meshgi's current research focuses on machine learning, generative AI, and designing, training, deploying, fine-tuning, interpreting, explaining, and orchestrating deep learning models on NLP and CV tasks.</p>
										<ul class="actions">
											<li><a href="files/Kourosh_Meshgi_CV_2024_04.pdf" class="button big">Curriculum Vitae</a></li>
										</ul>
									</div>
									<!-- <span class="image object">
										<img src="images/kourosh meshgi.jpg" alt="Kourosh" width="300"/>
									</span> -->
									
									
								</section>
								
								<script>
								var slideIndex = 1;
								showDivs(slideIndex);

								function plusDivs(n) {
								  showDivs(slideIndex += n);
								}

								function showDivs(n) {
								  var i;
								  var x = document.getElementsByClassName("mySlides");
								  if (n > x.length) {slideIndex = 1}    
								  if (n < 1) {slideIndex = x.length}
								  for (i = 0; i < x.length; i++) {
									 x[i].style.display = "none";  
								  }
								  x[slideIndex-1].style.display = "block";  
								}
								</script>
								
							<!-- Section -->
								<section id="publication">
									<header class="major">
										<h2 id="pub">Publications</h2>
									</header>
									<div class="features">
										<article>
											<span class="icon fa-language"></span>
											<div class="content">
												<h3>Natural Language Processing</h3>
												<ul class="fa-ul">
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, and S. Sekine, “What is the real intention behind this question? Dataset collection and intention classification”, In Proc. Of <a href="https://2023.aclweb.org/">ACL’23</a>, Toronto, Canada, Jul 2023.
													   	<a href="https://aclanthology.org/2023.acl-long.761"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, and S. Sekine, “Is this Question Real? Dataset Collection on Perceived Intentions and Implicit Attack Detection”, In Proc. of <a href="https://www2022.thewebconf.org/">WWW’22</a>, Lyon, France, Apr 2022.
														<a href="https://aclanthology.org/2023.acl-long.761.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, and S. Sekine, “Uncertainty Regularized Multi-Task Learning,” In Proc. of <a href="https://2022.aclweb.org/">ACL’22</a>, WASSA workshop, Dublin, Ireland, May 2022.
														<a href="https://aclanthology.org/2022.wassa-1.8/"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
																		
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, and S. Sekine, “Q-Learning Scheduler for Multi Task Learning Through the use of Histogram of Task Uncertainty,” In Proc. of <a href="https://2022.aclweb.org/">ACL’22</a>, RepL4NLP workshop, Dublin, Ireland, May 2022.
														<a href="https://aclanthology.org/2022.repl4nlp-1.2/"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fafa-check-square"></i>
													<strong>K. Meshgi</strong>, and M. S. Mirzaei, “Using Uncertainty for Multi-Domain Text Classification”, In <a href="https://exlingsociety.com/past-proceedings/">ExLing’20</a>, Athens, Greece, Oct 2020.
														<a href="https://doi.org/10.36505/ExLing-2020/11/0033/000448"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
																	
													<li><i class="fa-li fa fa-check-square"></i>
													S. Ebrahimi, <strong>K. Meshgi</strong>, S. Khadivi, S.E. Shiri Ahmad Abady, “Meta-level Statistical Machine Translation," in Proc. of <a href="https://www.aclweb.org/portal/node/2591">IJCNLP'13</a>, Nagoya, Japan, Oct 2013.
														<a href="http://aclweb.org/anthology/I/I13/I13-1164.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/stacksmt.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="stacksmt.html"><i class="fa fa-flask"></i></a> 
														<a href="https://www.semanticscholar.org/paper/Meta-level-Statistical-Machine-Translation-Ebrahimi-Meshgi/05524bbef1f3de989a24eee8db3a1bc51ba60735"><i class="fa fa-link"></i></a><a href="#"></a>
												</ul>
											</div>
										</article>
										<article>
											<span class="icon fa-low-vision"></span>
											<div class="content">
												<h3>Computer Vision</h3>
												<ul class="fa-ul">
													<li><i class="fa-li fa fa-spinner fa-spin"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Sekine, “Transformer-based Multi-Task Representation Learning for Robust Visual Tracking,” In Proc. of <a href="https://2024.ieeeicip.org/">ICIP’24</a>, IEEE, Abu Dhabi, UAE, Oct 2024 (submitted).
														<a href="#"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
															
													<li><i class="fa-li fa fa-check-square"></i>
													A. Kubo, <strong>K. Meshgi</strong>, S. Ishii, “Adaptive Correlation Filters for Visual Tracking via Reinforcement Learning,” <a href="https://www.springer.com/journal/10846">J. of Intelligent & Robotic Systems</a>, 2021.
														<a href="https://link.springer.com/article/10.1007/s10846-020-01273-2"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Oba, “Adversarial Semi-Supervised Multi-Domain Tracking,” In Proc. of <a href="https://accv2020.github.io/">ACCV’20</a>, IEEE, Kyoto, Japan, Nov 2020.
														<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Meshgi_Adversarial_Semi-Supervised_Multi-Domain_Tracking_ACCV_2020_paper.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, and S. Oba, “Leveraging Tacit Information Embedded in CNN Layers for Visual Tracking,” In Proc. of <a href="https://accv2020.github.io/">ACCV’20</a>, IEEE, Kyoto, Japan, Nov 2020.
														<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Meshgi_Leveraging_Tacit_Information_Embedded_in_CNN_Layers_for_Visual_Tracking_ACCV_2020_paper.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, and S. Sekine, “Uncertainty Regularized Multi-Task Learning for Text Classification,” in Proc. of <a href="https://www.acml-conf.org/2019/">ACML’19</a>, Weakly Supervised Learning Workshop, Nagoya, Japan, Nov 2019.
														<a href="#"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Oba, “Long and Short Memory Balancing in Visual Co-Tracking using Q-Learning,” in Proc. of <a href="https://2019.ieeeicip.org/">ICIP’19</a>, IEEE, Taipei, Taiwan, Sep 2019.
													    	<a href="https://ieeexplore.ieee.org/document/8803577"><i class="fa fa-file"></i></a> 
														<a href="bibtex/qact.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="qact.html"><i class="fa fa-flask"></i></a> 
														<a href="http://arxiv.org/abs/1902.05211"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Oba, “Information-Maximizing Sampling to Promote Tracking-by-Detection,” in Proc. of <a href="https://2018.ieeeicip.org/">ICIP’18</a>, IEEE, Athens, Greece, Oct 2018.
													    	<a href="https://ieeexplore.ieee.org/document/8451725/"><i class="fa fa-file"></i></a> 
														<a href="bibtex/imst.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="imst.html"><i class="fa fa-flask"></i></a> 
														<a href="https://arxiv.org/abs/1806.02523"><i class="fa fa-link"></i></a>
													
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Oba, S. Ishii, “Efficient Diverse Ensemble for Discriminative Co-Tracking,” in Proc. of <a href="http://cvpr2018.thecvf.com/">CVPR’18</a>, IEEE, Salt Lake City, USA, Jun 2018.
													    <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Meshgi_Efficient_Diverse_Ensemble_CVPR_2018_paper.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/dedt.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="dedt.html"><i class="fa fa-flask"></i></a> 
														<a href="https://arxiv.org/abs/1711.06564"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong> and S. Oba “Active Collaboration of Classifiers for Visual Tracking,” in G. Anbarjafari and S. Escalera (Eds) Human-Robot Interaction - Theory and Application, InTech Publication, ISBN 978-953-51-5611-6, 2018.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/act.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="act.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S.i. Maeda, S. Oba, S. Ishii, “Constructing a Meta-Tracker using Dropout to Imitate the Behavior of an Arbitrary Black-box Tracker,” <a href="https://www.journals.elsevier.com/neural-networks">Journal of Neural Networks</a> vol. 87, pp. 132-148, Elsevier, 2017.
													    	<a href="http://www.sciencedirect.com/science/article/pii/S0893608016301988"><i class="fa fa-file"></i></a> 
														<a href="bibtex/mimic.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="mimic.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1016/j.neunet.2016.12.009"><i class="fa fa-link"></i></a><a href="#"></a>  
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Oba, S. Ishii, “Efficient Asymmetric Co-Tracking using Uncertainty Sampling,” in Proc. of <a href="http://spsocmalaysia.org/icsipa2017/">ICSIPA’17</a>, IEEE, Kuching, Malaysia, Sep 2017. <a href="publication/ust/best.jpg">(Best paper award)</a>
													    	<a href="https://arxiv.org/pdf/1704.00083"><i class="fa fa-file"></i></a> 
														<a href="bibtex/ust.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="ust.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1109/ICSIPA.2017.8120614"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, S. Oba, S. Ishii, “Active Collaborative Ensemble Tracking,” in Proc. of <a href="http://www.avss2017.org/">AVSS’17</a>, IEEE, Lecce, Italy, Aug 2017.
													    	<a href="https://arxiv.org/abs/1704.08821"><i class="fa fa-file"></i></a> 
														<a href="bibtex/acet.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="acet.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1109/AVSS.2017.8078534"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Oba, S. Ishii, “Adversarial Sampling to Robustify Active Discriminative Co-Tracking,” in Proc. of <a href="http://cvim.ipsj.or.jp/MIRU2017/">MIRU’17</a>, Hiroshima, Japan, Aug 2017.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/asct.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="asct.html"><i class="fa fa-flask"></i></a> 
														<a href="http://cvim.ipsj.or.jp/MIRU2017/index.php?id=program-oral"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Oba, S. Ishii, “Efficient Version-Space Reduction for Visual Tracking,” in Proc. of <a href="http://www.computerrobotvision.org/2017/index.html">CRV’17</a>, IEEE, Vancouver, Canada, May 2017.
													    	<a href="http://ieeexplore.ieee.org/document/8287686/"><i class="fa fa-file"></i></a> 
														<a href="bibtex/qbst.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="qbst.html"><i class="fa fa-flask"></i></a> 
														<a href="https://arxiv.org/pdf/1704.00299"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Oba, S. Ishii, “Active Discriminative Tracking using Collective Memory,” in Proc. of <a href="http://www.mva-org.jp/mva2017/">MVA’17</a>, IEEE, Tokyo, Japan, May 2017.
													    	<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986879"><i class="fa fa-file"></i></a> 
														<a href="bibtex/cmt.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="cmt.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.23919/MVA.2017.7986879"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Oba, and S. Ishii, “Robust Discriminative Tracking via Query-by-Bagging,” in Proc. of <a href="http://avss2016.org/">AVSS’16</a>, Colorado Springs, USA, Aug 2016.
													    	<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986879"><i class="fa fa-file"></i></a> 
														<a href="bibtex/qbt.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="qbt.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1109/AVSS.2016.7738027"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S.i. Maeda, S. Oba, H. Skibbe, Y.Z. Li, S. Ishii, “Occlusion Aware Particle Filter Tracker to Handle Complex and Persistent Occlusions,” <a href="https://www.journals.elsevier.com/computer-vision-and-image-understanding/">Journal of Computer Vision and Image Understanding (CVIU)</a>, vol. 150, pp. 81-94, Elsevier 2016.
													    	<a href="http://dx.doi.org/10.1016/j.cviu.2016.05.011"><i class="fa fa-file"></i></a> 
														<a href="bibtex/oaptf.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="oaptf.html"><i class="fa fa-flask"></i></a> 
														<a href="http://dx.doi.org/10.1016/j.cviu.2016.05.011"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S.i. Maeda, S. Oba, and S. Ishii, “Data-driven Probabilistic Occlusion Mask to Promote Visual Tracking,” in Proc. of <a href="http://www.computerrobotvision.org/2016/index.html">CRV’16</a>, IEEE, British Columbia, Canada, Jun 2016.
													    	<a href="http://ieeexplore.ieee.org/document/7801519/"><i class="fa fa-file"></i></a> 
														<a href="bibtex/occmask.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="occmask.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1109/CRV.2016.19"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, S. Ishii, “The State-of-the-Art in Handling Occlusions for Visual Object Tracking,” <a href="https://www.jstage.jst.go.jp/browse/transinf/-char/en">IEICE Transactions on Information and Systems</a>, vol. E98-D, no. 7, pp. 1260-1274, IEICE 2015.
													    	<a href="https://www.jstage.jst.go.jp/article/transinf/E98.D/7/E98.D_2014EDR0002/_pdf/-char/en"><i class="fa fa-file"></i></a> 
														<a href="bibtex/occmask.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="occmask.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1587/transinf.2014EDR0002"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, and S. Ishii, “Expanding Histogram of Colors with Gridding to Improve Tracking Accuracy,” in Proc. of <a href="http://www.mva-org.jp/mva2015/">MVA’15</a>, IEEE, Tokyo, Japan, May 2015.
													    	<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153234"><i class="fa fa-file"></i></a> 
														<a href="bibtex/hocx.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="hocx.html"><i class="fa fa-flask"></i></a>  
														<a href="http://doi.org/10.1109/MVA.2015.7153234"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-square"></i>
													<strong>K. Meshgi</strong>, S. Maeda, S. Oba, S. Ishii, “Fusion of Multiple Cues from Color and Depth Domains using Occlusion Aware Bayesian Tracker, ” in IEICE Tech. Rep., vol. 113, no. 500, <a href="http://www.ieice.org/ken/program/index.php?tgs_regid=376c1c5a5525e7d39bc18940be9e96fab790c95a292ea92f09e361e959c1c642&lang=eng">NC2014</a>-22, pp. 127-132, Mar 2014.
													    	<a href="https://pdfs.semanticscholar.org/29ba/f10a11f7f142cddf10582c5d8258e456d741.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/oapft1.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="oapft.html"><i class="fa fa-flask"></i></a>  
														<a href="http://www.ieice.org/ken/paper/20140318BBM3/eng/"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-square"></i>
													<strong>K. Meshgi</strong>, Y.Z. Li, S. Oba, S. Maeda, S. Ishii, “Enhancing Probabilistic Appearance-Based Object Tracking with Depth Information: Object Tracking under Occlusion,” in IEICE Tech. Rep., vol. 113, no. 197, <a href="http://ibisml.org/ibis2013/">IBISML2013</a>-22, pp. 85-91, Sep 2013. 
													    	<a href="https://pdfs.semanticscholar.org/5e86/77eb21c3a5d24c52bcb93404416f7eeebc31.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/oapft2.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="occmask.html"><i class="fa fa-flask"></i></a>  
														<a href="http://www.ieice.org/ken/paper/20130902SBfY/eng/"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-mortar-board"></i>
													<strong>K. Meshgi</strong>, “Particle Filter-based Tracking to Handle Persistent and Complex Occlusions and Imitate Arbitrary Black-box Trackers,” Ph.D. Dissertation, Kyoto University, Sep 2015.
													    	<a href="publication/project-phd/phd.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/phd.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="project-phd.html"><i class="fa fa-flask"></i></a> 
														<a href="https://repository.kulib.kyoto-u.ac.jp/dspace/bitstream/2433/202747/2/djohk00594.pdf"><i class="fa fa-link"></i></a>
													
												</ul>
											</div>
										</article>
										<article>
											<span class="icon fa-cogs"></span>
											<div class="content">
												<h3>Computational Linguistics</h3>
												<ul class="fa-ul">
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, and <strong>K. Meshgi</strong>, “The use of machine learning in developing learner-adaptive tools for second language acquisition”, In Proc. of <a href="https://vigdis.hi.is/en/events/eurocall-2023/">EuroCALL’23</a>, Reykjavík, Iceland, Aug 2023.
														<a href="https://lnu.diva-portal.org/smash/get/diva2:1846786/FULLTEXT01.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, and <strong>K. Meshgi</strong>, “Self-regulated listening experience with smart captioning”, In Proc. of <a href="https://vigdis.hi.is/en/events/eurocall-2022/">EuroCALL’23</a>, Reykjavík, Iceland, Aug 2022.
														<a href="http://ocs.editorial.upv.es/index.php/EuroCALL/EuroCALL2023/paper/view/16996/8327"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, and <strong>K. Meshgi</strong>, “Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources”, In Proc. of <a href="https://www.interspeech2021.org/">Interspeech’21</a>, Brno, Czechia, Aug 2021.
														<a href="https://www.sciencedirect.com/science/article/abs/pii/S0885230816303473"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, and <strong>K. Meshgi</strong>, “Facilitating L2 listening through automatic detection of speech and lexical difficulties”, In Proc. of <a href="https://whova.com/web/euroc1_202108/">EuroCALL’21</a>, Paris, France, Aug 2021.
														<a href="https://files.eric.ed.gov/fulltext/ED617993.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>


													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, and T. Nishida, “Promoting common ground building in L2 cross-cultural conversations”, In Proc. of <a href="https://whova.com/web/euroc1_202108/">EuroCALL’21</a>, Paris, France, Aug 2021.
														<a href="https://files.eric.ed.gov/fulltext/ED617916.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, and T. Nishida “Sentence Complexity as an Indicator of L2 Learner’s Listening Difficulty,” In Proc. of <a href="https://cip.ku.dk/english/events/previous_events/eurocall-2020/">EuroCALL’20</a>, Copenhagen, Denmark, Aug 2020.
														<a href="https://files.eric.ed.gov/fulltext/ED611114.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, and T. Nishida, “A Situation Creation System to Enable Experiential Learning in Virtual Worlds for Developing Cross-Cultural Competencies,” In Proc. of <a href="https://cip.ku.dk/english/events/previous_events/eurocall-2020/">EuroCALL’20</a>, Copenhagen, Denmark, Aug 2020.
														<a href="https://files.eric.ed.gov/fulltext/ED611113.pdf"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, and <strong>K. Meshgi</strong>, “A Data-driven Caption for L2 Listening”, In Proc. of <a herf="https://exlingsociety.com/past-proceedings/">ExLing’20</a>, Athens, Greece, Oct 2020.
														<a href="https://doi.org/10.36505/ExLing-2020/11/0042/000457"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, “Learner Adaptive Partial and Synchronized Caption for L2 Listening Skill Development,” in Proc. of <a href="https://sites.uclouvain.be/eurocall2019/">EuroCALL’19</a>, Louvain-la-Neuve, Belgium, Aug 2019.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscec19.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, “Toward Adaptive Partial and Synchronized Caption to Facilitate L2 Listening,” in Proc. of <a href="https://www.j-let.org/fleatvii/">FLEAT VII</a>, Tokyo, Japan, Aug 2019.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscfl19.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, “A comprehensive word difficulty index for L2 listening,” in Proc. of <a href="http://exlingworkshop.com/">ExLing’18</a>, Paris, France, Aug 2018.
													    	<a href="http://exlingworkshop.com/images/ExLing-2018/ExLing-2018-preliminary-proceedings.pdf#page=86"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscex18.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
													
													<li><i class="fa-li fa fa-check-square"></i>
													<strong>K. Meshgi</strong>, M. S. Mirzaei, “A Data-driven Approach to Generate Partial and Synchronized Caption for Second Language Listeners,” in Proc. of <a href="http://worldcall5.org/">WorldCALL’18</a>, Concepcion, Chile, Nov 2018.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscwc18.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, “Automatic Scaffolding for L2 Listeners by Leveraging Natural Language Processing,” in Proc. of <a href="https://www.jyu.fi/en/congress/eurocall2018">EuroCALL’18</a>, Jyväskylä, Finland, Aug 2018.
													    	<a href="#"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscec18.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, T. Kawahara, “Exploiting Automatic Speech Recognition Errors to Enhance Partial and Synchronized Caption for Facilitating Second Language Listening,” <a href="https://www.journals.elsevier.com/computer-speech-and-language">Computer Speech and Language Journal</a>, vol. 49, pp. 17-36, Elsevier 2018.
													    	<a href="http://www.sciencedirect.com/science/article/pii/S0885230816303473"><i class="fa fa-file"></i></a> 
														<a href="bibtex/psc2j.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1016/j.csl.2017.11.001"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, Y. Akita, T. Kawahara, “Partial and synchronized captioning: A new tool to assist learners in developing second language listening skill,” <a href="https://www.cambridge.org/core/journals/recall">ReCALL Journal</a>, vol. 29(2), pp. 178-199, Cambridge University Press 2017.
													    	<a href="https://www.cambridge.org/core/journals/recall/article/div-classtitlepartial-and-synchronized-captioning-a-new-tool-to-assist-learners-in-developing-second-language-listening-skilldiv/9394E3F0AD25FD5F32895F965F7ECCAD"><i class="fa fa-file"></i></a> 
														<a href="bibtex/psc1j.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1017/S0958344017000039"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, T. Kawahara, “Detecting listening difficulty for second language learners using Automatic Speech Recognition errors,” in Proc. of <a href="http://www.slate2017.org/">SLaTE’17</a>, Stockholm, Sweden, Aug 2017.
													    	<a href="http://www.slate2017.org/papers/SLaTE_2017_paper_44.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/psc2slate.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://dx.doi.org/10.21437/SLaTE.2017-27"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, T. Kawahara, “Listening Difficulty Detection to Foster Second Language Listening with Partial and Synchronized Caption,” in Proc. of <a href="http://www.eurocall2017.uk/">EuroCALL’17</a>, Southampton, England, Aug 2017.
													    	<a href="https://research-publishing.net/publication/chapters/978-2-490057-04-7/715.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscec17.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.14705/rpnet.2017.eurocall2017.715"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, T. Kawahara, “Leveraging automatic speech recognition errors to detect challenging speech segments in TED talks,” in Proc. of <a href="https://eurocall2016.org/">EuroCALL’16</a>, Limassol, Cyprus, Aug 2016.
													    	<a href="https://research-publishing.net/publication/chapters/978-1-908416-44-5/581.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscec16.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.14705/rpnet.2016.EUROCALL2016.9781908416445"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, T. Kawahara, “Automatic speech recognition errors as a predictor of L2 listening difficulties,” in Proc. of <a href="http://coling2016.anlp.jp/">Coling’16</a> (<a href="https://sites.google.com/site/cl4lc2016/home">CL4LC Workshop</a>), Osaka, Japan, Nov 2016.
													    	<a href="http://www.aclweb.org/anthology/W/W16/W16-4122.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/psc2cl4lc.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.1016/j.csl.2017.11.001"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, Y. Akita, T. Kawahara, “Errors in Automatic Speech Recognition versus Difficulties in Second Language Listening, ” in Proc. of <a href="http://www.eurocall2015.it/en/sistemacongressi/eurocall-2015/website/home/">EuroCALL’15</a>, Padova, Italy, Aug 2015.
													    	<a href="https://reference.research-publishing.net/publication/chapters/978-1-908416-29-2/367.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/pscec15.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="psc.html"><i class="fa fa-flask"></i></a> 
														<a href="https://doi.org/10.14705/rpnet.2015.000367"><i class="fa fa-link"></i></a>
												</ul>
											</div>
										</article>
										<article>
											<span class="icon fa-signal"></span>
											<div class="content">
												<h3>Machine Learning</h3>
												<ul class="fa-ul">
													<li><i class="fa-li fa fa-check-square"></i>
													M. S. Mirzaei, <strong>K. Meshgi</strong>, E. Frigo, and T. Nishida, “AnimGAN: A Spatiotemporally-Conditioned Generative Adversarial Network for Character Animation,” in Proc. of <a href="https://2020.ieeeicip.org/">ICIP’20</a>, IEEE, Abu Dhabi, UAE, Oct 2020.
														<a href="#"><i class="fa fa-file"></i></a> 
														<a href="#"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-flask"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>

													<li><i class="fa-li fa fa-check-square"></i>
													S. MasoumZadeh, <strong>K. Meshgi</strong>, S. Shiry, G. Taghizadeh, “FQL-RED: An Adaptive Scalable Schema for Active Queue Management,” Int'l. J. of Network Mgmt (IJNM), vol. 21, pp.157-167 Wiley, 2011. 
													    	<a href="http://onlinelibrary.wiley.com/doi/10.1002/nem.755/epdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/fqlred.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="fqlred.html"><i class="fa fa-flask"></i></a> 
														<a href="http://doi.org/10.1002/nem.755"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													S. MasoumZadeh, G. Taghizadeh, <strong>K. Meshgi</strong>, S. Shiry, “Deep Blue: A Fuzzy Q-Learning Enhanced Active Queue Management Scheme,” in Proc. of Int'l. Conf. on Adaptive and Intelligent Systems (ICAIS'09), Klagenfurt, Austria, 2009. 
													    	<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328070"><i class="fa fa-file"></i></a> 
														<a href="bibtex/deepblue.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="deepblue.html"><i class="fa fa-flask"></i></a> 
														<a href="http://doi.org/10.1109/ICAIS.2009.17"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-check-square"></i>
													S. MasoumZadeh, <strong>K. Meshgi</strong>, S. Shiry, “Adaptive Mutation in Evolution Strategy using Fuzzy Q-Learning,” in Proc. of 13th Iran Computer Association Conf. (ICCSC2008), Kish, Iran, 2008.
													    	<a href="publication/fql/fqlmutation.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/fqlmutation.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="#"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-mortar-board"></i>
													<strong>K. Meshgi</strong>, “Brain Inspired Face Detection,” M.Sc. Dissertation, Tehran PolyTechnic, Oct 2010.
													    	<a href="publication/bfd/msc.pdf"><i class="fa fa-file"></i></a> 
														<a href="bibtex/msc.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="bfd.html"><i class="fa fa-flask"></i></a> 
												</ul>
											</div>
										</article>
										<article>
											<span class="icon fa-drupal"></span>
											<div class="content">
												<h3>Robotics</h3>
												<ul class="fa-ul">
													<li><i class="fa-li fa fa-check-square"></i>
													S. Soleimanpour, S. Shiry, <strong>K. Meshgi</strong>, “Sensor Fusion in Robot Localization using DS-Evidence Theory with Conflict Detection using Mahalanobis Distance,” Proc. of 7th IEEE Int'l. Conf. on Cybernetic Intelligent Systems (<a href="http://www.cybernetic.org.uk/cis2008">CIS'2008</a>), United Kingdom, 2008.
													    	<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4798964"><i class="fa fa-file"></i></a> 
														<a href="bibtex/sensorfusion.txt"><i class="fa fa-quote-right"></i></a> 
														<a href="sensorfusion.html"><i class="fa fa-flask"></i></a> <a href="http://doi.org/10.1109/UKRICIS.2008.4798964"><i class="fa fa-link"></i></a>
														
													<li><i class="fa-li fa fa-square"></i>
													Nemesis 2010 Team Description, <a href="http://robocup2010.org/">RobuCup 2010</a>.
													    	<a href="publication/robocup/tdp-nemesis-2010-long.pdf"><i class="fa fa-file"></i></a>
												</ul>
												<p><br/></p>
												<p>
												<strong>Legend:</strong>   <i class="fa fa-check-square">  Peer-reviewed</i>, <i class="fa fa-square">  Not peer-reviewed</i>, <i class="fa fa-spinner">  Under review/publication</i>, <i class="fa fa-mortar-board">  Thesis</i>
												</p>
												
											</div>
										</article>
									</div>
								</section>
								
								<!-- Section -->
								<section id="activities">
									<header class="major">
										<h2 id="activities">Activities</h2>
									</header>
									<div class="features">
										<article>
											<span class="icon fa-diamond"></span>
											<div class="content">
												<h3>Honors</h3>
												<ul>
													<li> Awarded <a href="https://nrid.nii.ac.jp/en/nrid/1000080774835/">JSPS Kakenhi</a> (Grants-in-Aid for Scientific Research), Japan Society for the Promotion of Science, 2017.</li>
													<li> Won <a href="http://ict-nw.i.kyoto-u.ac.jp/ict-innovation/11th/">ICT Innovation Award</a> for <a href="psc.html">PSC</a>, Kyoto University, 2017.
													<li> Awarded as <a href="http://www.nedo.go.jp/news/press/AA5_100431.html">Japan Ministry of Economy, Trade and Industry Prize for Winning NEDO Project</a>, as a part of R&D Team, 3D MEDiA Co. Ltd.</a>, 2015.
													<li> Received Japan Government Monbokagakusho (MEXT) Scholarship from the Ministry of Education, Culture, Sports, Science and Technology, Government of Japan, 2011-2014.
													<li> Achieved 3rd Place of Int'l. <a href="http://www.robocup2005.org/news/results.aspx">RoboCup 2005 Competitions</a>, Soccer Coach Simulation League, Member of Kasra Team, Osaka, Japan, 2005.
													<li> Recognized as an Exceptional Talent during Master's Program by Amirkabir University of Technology, 2010 (Ranked 1st in Fall 2008 & 2nd in Spring 2008 semesters in Artificial Intelligence Dept.)
												</ul>
											</div>
										</article>
										<article>
											<span class="icon  fa-institution"></span>
											<div class="content">
												<h3>Affiliations</h3>
												<ul>
													<li> <a href="https://www.ieee.org/index.html">IEEE</a> Member (2009~), IEEE <a href="https://securesso.ieee.org/ieeevendorsso/loginservice">Robotics and Automation Society</a> (2010~), <a href="http://rc.signalprocessingsociety.org/">Signal Processing Society</a> (2015~), IEEE <a href="http://www.ieeesmc.org/">Systems, Man, and Cybernetics Society</a> (2017~)
													<li> <a href="https://calico.org/">CALICO</a> (Computer-Assisted Language Instruction Consortium), 2015~
													<li> Robocup Soccer Simulation Team Nemesis (2D Sim, 2010), Aria 3D (3D Sim, 2006~2008), and Kasra (2D Coach Sim, 2005) 
													<li> <a href="http://www.cipprs.org/">CIPPRS</a> (Canadian Image Processing and Pattern Recognition Society) Member, 2017~
													<li> <a href="http://www.ieice.org/eng/index.html">IEICE</a> (Institute of Electronics, Information and Communication Engineers) Member, 2017~
												</ul>
											</div>
										</article>
										<article>
											<span class="icon fa-font-awesome"></span>
											<div class="content">
												<h3>Services</h3>
												<ul>
													<li> Served as a reviewer for IEEE Trans. Of Multimedia, ACM Computing Surveys (CSUR), Journal of Neural Computing and Applications (NCAA), MDPI Journals (Sensors, Future Internet, Electronics, Information) Journal, IEICE Tran. on Information and Systems, Journal of Computer and Communications (JCC), Journal of ICT Research and Applications, and ACL’23, AAAI’23, AACL-IJNLP’22, ACCV’22, COLING’22, CVPR’20, ICML’20, ECCV’20, WiNLP’20, ACL’20, IJCNLP’20, WiNLP’19, ICAIR’16, ICCTS’17 ISCAI’18, FSDM’18, SNSP’18 conferences (<a href="http://publons.com/a/1457709/">Publons page:</a> Top Reviewer of 2019)
													<li> Organizing Committee of Widening NLP Workshop, WiNLP’2020, Seattle, USA, 2020;</li>
													<li> Organizing Committee of Widening NLP Workshop, WiNLP’2020, Seattle, USA, 2020;
													<li> Editorial Member of <a href="http://gavinpublishers.com/journal-of-robotic-engineering-automation-editorial-board/">Journal of Robotics Engineering and Automation Technology</a>, Gavin Publishers, USA, 2017-present.
													<li> Editorial Member of <a href="http://ojs.whioce.com/index.php/phci/about/editorialTeam">Progress in Human Computer Interaction Journal</a>, Whioce Publishing Pte. Ltd., 2018-present. 
													<li> <a href="http://icsai.sdju.edu.cn/">Organizing Committee of Int’l Conference on Systems and Informatics</a>, ICSAI’17, Hangzhou, China, 2017.
													<li> <a href="http://www.ciis2017.org/">Organizing Committee of Int’l Conference on Computing Intelligence and Information System</a>, CIIS’17, Nanjing, China, 2017.
													<li> <a href="http://www.cst2017.org/">Organizing Committee of Int’l Conference on Computer Science and Technology</a>, ICCST’17, Guilin, China, 2017.
													<li> Scientific Committee of Intl. Computer Society of Iran Computer Conference, <a href="https://iktconference.ir/2020/Home/Content?id=53">IKT'20</a>, Tehran, Iran, 2020.
													<li> Organizing Committee of Intl. Computer Society of Iran Computer Conference, CSICC'09, Tehran, Iran, 2009.
													<li> Organizing Committee of 1st and 2nd Conf. of Information & Knowledge Technology, IKT 2003 & IKT 2005, Tehran, Iran, 2003-2005.
												</ul>
											</div>
										</article>
									</div>
								</section>

							<!-- Section -->
								<section id="projects">
									<header class="major">
										<h2>Projects</h2>
									</header>
									<div class="posts">
										<article>
											<a href="dedt.html" class="image"><img src="publication/dedt/dedt.png" alt="DEDT" /></a>
											<h3>Efficient Diverse Ensemble for Discriminative Co-Tracking (DEDT)</h3>
											<p>An active co-tracker with self-organizing committee of classifiers, considering diversity for model update</p>
											<ul class="actions">
												<li><a href="dedt.html" class="button">More</a></li>
											</ul>
										</article>										
										<article>
											<a href="acet.html" class="image"><img src="publication/acet/acet.png" alt="ACET" /></a>
											<h3>Adversarial Ensemble Co-Tracker (ACET)</h3>
											<p>An active self-correcting committee of classifiers to perform collaborative tracking</p>
											<ul class="actions">
												<li><a href="acet.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="dqcf.html" class="image"><img src="publication/dqcf/dqcf.jpg" alt="DQCF" /></a>
											<h3>Deep Q-Learning for Correlation Filter Tracking (DQCF)</h3>
											<p>A correlation filter tracker with reinforcement-learning-tuned learning rate</p>
											<ul class="actions">
												<li><a href="dqcf.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="qbst.html" class="image"><img src="publication/qbst/qbst.jpg" alt="QBST" /></a>
											<h3>Query-by-Boosting Tracker (QBST)</h3>
											<p>a committee of weak classifiers in a Boosting framework</p>
											<ul class="actions">
												<li><a href="qbst.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="cmt.html" class="image"><img src="publication/cmt/cmt.png" alt="CMT" /></a>
											<h3>Collective Memory Tracker (CMT)</h3>
											<p>a committee of classifiers with the same data but different memory spans</p>
											<ul class="actions">
												<li><a href="cmt.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="qbt.html" class="image"><img src="publication/qbt/qbt.png" alt="QBT" /></a>
											<h3>Query-by-Bagging Tracker (QBT)</h3>
											<p>a committee of classifiers with partial knowledge</p>
											<ul class="actions">
												<li><a href="qbt.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="mimic.html" class="image"><img src="publication/mimic/mimic.png" alt="" /></a>
											<h3>Mimic Tracker (MIMIC)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="mimic.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="imst.html" class="image"><img src="publication/imst/imst.png" alt="IMST" /></a>
											<h3>Information-Maximizing Sampling Tracker (IMST)</h3>
											<p>A novel sampling technique to provide most informative samples for the classifiers to learn the target/non-target ever-changing boundary</p>
											<ul class="actions">
												<li><a href="imst.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="occmask.html" class="image"><img src="publication/occmask/occmask.png" alt="OCCMASK" /></a>
											<h3>Data-Driven Probabilistic Occlusion Mask (OccMask)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="occmask.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="hocx.html" class="image"><img src="publication/hocx/hocx.png" alt="HOCX" /></a>
											<h3>Expanding Histogram of Colors with Gridding (HOCx)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="hocx.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="oapft.html" class="image"><img src="publication/oapft/oapft.png" alt="OAPFT" /></a>
											<h3>Occlusion Aware Particle Filter Tracker (OAPFT)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="asct.html" class="image"><img src="publication/asct/asct.png" alt="ASCT" /></a>
											<h3>Adversarial Sampling Co-Tracker (ASCT)</h3>
											<p>An active co-tracker in which the main classifier is robustified against its adversarial examples by the assistance of the auxiliary detector</p>
											<ul class="actions">
												<li><a href="asct.html" class="button">More</a></li>
											</ul>
										</article>
										<!--<article>
											<a href="occsurvay.html" class="image"><img src="images/pic03.jpg" alt="OCCSURVAY" /></a>
											<h3>The State-of-the-Art in Handling Occlusions for Visual Object tracking (OccSurvey)</h3>
											<p>A report on trending literature of occlusion handling in online visual tracking including solutions, datasets, benchmarks, and criteria. </p>
											<ul class="actions">
												<li><a href="occsurvay.html" class="button">More</a></li>
											</ul>
										</article>-->
										<article>
											<a href="psc.html" class="image"><img src="publication/psc/psc2.png" alt="PSC" /></a>
											<h3>Partial and Synchronized Caption (PSC)</h3>
											<p>A novel method of captioning as a listening tool for the second language learners in which automatically selected set of words in a caption where words are synced to their corresponding speech signals. <a haref="http://www.ar.media.kyoto-u.ac.jp/psc/#DEMO">Click here</a> (the official project page) to see demonstration videos on how PSC works.</p>
											<ul class="actions">
												<li><a href="psc.html" class="button">More</a></li>
											</ul>
										</article>
										<article>
											<a href="qact.html" class="image"><img src="publication/qact/qact.png" alt="" /></a>
											<h3>Long and Short Memory Balancing in Visual Co-Tracking using Q-Learning (QACT)</h3>
											<p>An adaptive active-co tracker, that uses Q-learning to balance the short vs. long memory usage and speed vs. accuracy trade-off</p>
											<ul class="actions">
												<li><a href="qact.html" class="button">More</a></li>
											</ul>
										</article>
										<!--<article>
											<a href="#" class="image"><img src="images/pic05.jpg" alt="" /></a>
											<h3>Brain Inspired Face Detection (B-FD)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>
										</article>-->
										
										<!--<article>
											<a href="#" class="image"><img src="images/pic06.jpg" alt="" /></a>
											<h3>Fuzzy Q-Learning (FQL)</h3>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
											<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>
										</article>-->
										<!--<article>
											<a href="#" class="image"><img src="images/pic07.jpg" alt="" /></a>
											<h3>Virtual Reality Conversation Envinsioner (VRCE)</h3>
											<p>We aim to facilitate common ground building and smoothen communication by envisioning the tacit dimensions of interactions and designing agents that can smoothly participate in the conversation with the human. Our present research also focuses designing effective tools for facilitating cross-cultural communications.</p>
											<ul class="actions">
												<li><a href="#" class="button">More</a></li>
											</ul>
										</article> -->
									</div>
								</section>
							<!-- Section -->
								<section id="contact">
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<!-- <p>For additional info and data please visit my <a href="old/">old</a> homepage</p> -->
									<ul class="contact">
									<ul class="contact">
										<li class="fa-envelope-o"><a href="#">kourosh.meshgi [at] riken.jp</a></li>
										<!--<li class="fa-phone">+81 (50) 35005357</li> -->
										<li class="fa-home">RIKEN AIP, Mitsui Building 15F, 1-4-1 Nihonbashi,Chuo-ku, Tokyo </li>
									</ul>
									<div lass="major">
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3240.775329456104!2d139.77244341525872!3d35.68253368019396!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x601889564cd58693%3A0x3b40159699057160!2sRIKEN%20Center%20for%20Advanced%20Intelligence%20Project%20(AIP)!5e0!3m2!1sen!2sjp!4v1675151646399!5m2!1sen!2sjp" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
									</div>
								</section>
						</div>
					</div>

		

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
